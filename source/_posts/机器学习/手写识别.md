---
title: 手写识别
categories:
  - 机器学习
  - 应用实例
tags:
  - 图像识别
  - KNN
  - 感知机
cover: 'https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508230144.png'
abbrlink: 8cb4e0ba
date: 2020-05-08 20:31:58
---

# 引言
>图像识别 Image Recognition 是 指利用计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对像的技术 。
>
>图像识别的发展经历了三个阶段：文字识别、数字图像处理与识别、物体识别 。机器学习 领域一般将此类识别问题转化为分类问题。

## 手写识别
>手写识别是常见的图像识别任务。计算机通过手写体图片来识别出图片中的 字，与印刷字体不同的 是，不同人的手写体风格迥异 ，大小不一造成了计算机对手写识别任务的一些困难。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508203532.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">0和4的六种写法</div>
</center>

### 数据集
> 数字手写体识别由于其有限的类别（ 0~9 共 10 个数字）成为了相对简单的手写识别任务。 DBRHD 和 MNIST 是常用的两个数字手写识别数据集。
#### MNIST
[MNIST](http://yann.lecun.com/exdb/mnist)是一个包含数字 0~9 的手写体图片数据集，图片已归一化为以手写数字为中心的 28*28 规格的图片。 MNIST 由训练集与测试集两个部分组成，各部分规模如下：
- 训练集：60,000 个手写体图片及对应标签
- 测试集：10,000 个手写体图片及对应标签

##### MNIST数据集的手写数字样例：
- MNIST数据集中的每一个图片由 28 28 个像素点组成
- 每个像素点的值区间为 0~255，0 表示白色， 255 表示黑色。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508204225.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">MNIST手写数字样例</div>
</center>
#### DBRHD
[DBRHD](https://archive.ics.uci.edu/ml/datasets/PenBased+Recognition+of+Handwritten+Digits)（Pen-Based Recognition of Handwritten Digits Data Set）是 UCI 的机器学习中心提供的数字手写体数据库，数据集包含大量的数字 0~9 的手写体图片，这些图片来源于 44 位不同的人的手写数字，图片已归一化为以手写数字为中心的 32 32 规格的图片。 

DBRHD 的训练集与测试集组成如下：
- 训练集：7494 个手写体图片及对应标签，来源于 40 位手写者
- 测试集：3498 个手写体图片及对应标签，来源于 14 位手写者

##### DBRHD数据集样例
- 去掉了图片颜色等复杂因素，将手写体数字图片转化为训练数据为大小 32 32 的 文本矩阵
- 空白区域使用 0 代表，字迹区域使用 1 表示。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508204937.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">DBRHD手写数字样例</div>
</center>
### 模型实例
已有许多模型在MNIST或DBRHD数据集上进行了实验，有些模型对数据集进行了偏斜矫正，甚至在数据集上进行了人为的扭曲、偏移、缩放及失真等操作以获取更加多样性的样本，使得模型更具有泛化性。

常用于数字手写体的分类器：
>1. 线性分类器 
>1. K最近邻分类器
>1. Boosted Stumps 
>1. 非线性分类器
>1. SVM 
>1. 多层感知机
>1. 卷积神经网络
-------------------------------------------------------
下面介绍多层感知机神经网络和KNN分类算法对手写识别的实例应用
# 多层感知器
## 任务
利用sklearn来训练一个简单的全连接神经网络，即多层感知机Multilayer perceptron MLP）用于识别数据集DBRHD的手写数字。
### MLP的输入
- DBRHD数据集的每个图片是一个由0或1组成的 32 32 的文本矩阵；
- 多层感知机的输入为图片矩阵展开的1*1024个神经元 。

### MLP的输出——one hot vectors
- 一 个 one hot 向量除了某一位的数字是 1 以外其余各维度数字都是 0 。
- 图片标签将 表示成一个只有在第 n 维度（从 0 开始）数字为 1 的 10 维向量。比如，标签 0 将表示 成 1,0,0,0,0,0,0,0,0,0,0 。即，MLP输出层具有10个神经元。

### MLP结构
- MLP 的输入与输出层，中间隐藏层的层数和神经元的个数设置都将影响该 MLP 模型的准确率。
- 在本实例中，我们只设置一层隐藏层，在后续实验中比较该隐藏层神经元个数为 50、100、200 时的 MLP 效果。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508215309.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">多层感知机（MLP）结构</div>
</center>

## 实验步骤
1. 建立工程并导入 sklearn 包
1. 加载训练数据
1. 训练神经网络
1. 测试集评价
```python
# 建立工程并导入 sklearn 包
import numpy as np				#导入numpy工具包
from os import listdir 			#使用listdir模块，用于访问本地文件
from sklearn.neural_network import MLPClassifier 
# 加载训练集数据
def img2vector(fileName):    
    retMat = np.zeros([1024],int) 	#定义返回的矩阵，大小为1*1024
    fr = open(fileName)           	#打开包含32*32大小的数字文件 
    lines = fr.readlines()        	#读取文件的所有行
    for i in range(32):           	#遍历文件所有行
        for j in range(32):       	#并将01数字存放在retMat中     
            retMat[i*32+j] = lines[i][j]    
    return retMat
 
def readDataSet(path):    
    fileList = listdir(path)    	#获取文件夹下的所有文件 
    numFiles = len(fileList)    	#统计需要读取的文件的数目
    dataSet = np.zeros([numFiles,1024],int) #用于存放所有的数字文件
    hwLabels = np.zeros([numFiles,10])      #用于存放对应的one-hot标签
    for i in range(numFiles):   	#遍历所有的文件
        filePath = fileList[i]  	#获取文件名称/路径      
        digit = int(filePath.split('_')[0]) #通过文件名获取标签      
        hwLabels[i][digit] = 1.0    #将对应的one-hot标签置1
        dataSet[i] = img2vector(path +'/'+filePath) #读取文件内容   
    return dataSet,hwLabels
 
#读取数据，将图片存放在train_dataSet，标签存放在train_hwLabels
train_dataSet, train_hwLabels = readDataSet('trainingDigits')
# 构建神经网络
# 设置网络的隐藏层数、各隐藏层神经元个数、激活函数、学习率、优化方法、最大迭代次数。
# 设置含 100 个神经元的隐藏层。
# hidden_layer_sizes 存放的是一个元组，表示第 i 层隐藏层里神经元的个数
# 使用 logistic 激活函数和 adam 优化方法，并令初始学习率为 0.0001
clf = MLPClassifier(hidden_layer_sizes=(100,),
                    activation='logistic', solver='adam',
                    learning_rate_init = 0.0001, max_iter=2000)
print(clf)
# 训练构建好的神经网络
# fit函数能够根据训练集及对应标签集大小自动设置多层感知机的输入与输出层的神经元个数。
# 例如 train_dataSet为 n*1024 的矩阵， train_hwLabels为 n*10 的矩阵，则fit函数将 MLP的输入层神经元个数设为1024 ，输出层神经元个数为10
clf.fit(train_dataSet,train_hwLabels) 
 
#加载测试集数据
dataSet,hwLabels = readDataSet('testDigits') 
res = clf.predict(dataSet)   #对测试集进行预测
error_num = 0                #统计预测错误的数目
num = len(dataSet)           #测试集的数目
for i in range(num):         #遍历预测结果
    #比较长度为10的数组，返回包含01的数组，0为不同，1为相同
    #若预测结果与真实结果相同，则10个数字全为1，否则不全为1
    if np.sum(res[i] == hwLabels[i]) < 10: 
        error_num += 1                     
print("总的数据:",num,"错误数据:",error_num,"错误率:",error_num / float(num))
```

## 实验效果
### 隐藏层神经元个数影响
运行隐藏层神经元个数为50、100、200的多层感知机，对比实验效果：
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508221353.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">不同神经元个数的实验效果</div>
</center>

- 随着隐藏层神经元个数的增加， MLP 的正确率持上升趋势；
- 大量的隐藏层神经元带来的计算负担与对结果的提升并不对等，因此，如何选取合适的
隐藏神经元个数是一个值得探讨的问题。
### 迭代次数影响分析
我们设隐藏层神经元个数为100，初始学习率为0.0001，最大迭代次数分别为500 、1000、1500、2000, 结果如下：
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508221725.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">不同迭代次数的实验效果</div>
</center>

- 过小的迭代次数可能使得 MLP 早停，造成较低的正确率。
- 当最大迭代次数 >1000 时，正确率基本保持不变，这说明 MLP 在第 1000 迭代时已收敛，剩余的迭代次数不再进行。
- 一般设置较大的最大迭代次数来保证多层感知机能够收敛，达到较高的正确率。
### 学习率影响分析
改用随机梯度下降优化算法即将MLPclassifer的参数（ solver sgd ’, 设隐藏层神经元个数为 100，最大迭代次数为2000，学习率分别为0.1、0.01、0.001、0.0001结果如下：
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508221953.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">不同学习率的实验效果</div>
</center>

## 结论
- **较小的学习率带来了更低的正确率**，这是因为较小学习率无法在 2000 次迭代内完成收敛，而步长较大的学习率使得 MLP 在 2000 次迭代内快速收敛到最优解。因此，较小的学习率一般要配备较大的迭代次数以保证其收敛。
-----------------------------
# KNN
## 任务
- 本实例利用sklearn来训练一个K最近邻（k-Nearest Neighbor KNN分类器，用于识别数据集DBRHD的手写数字。
- 比较KNN的识别效果与多层感知机的识别效果。

## 实验步骤
1. 建立工程并导入sklearn包
2. 加载训练数据
3. 构建KNN分类器
4. 测试集评价
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508223233.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">KNN算法示意图</div>
</center>

tips：代码大部分和[多层感知机-手写识别](https://dengsong123.com/2020/05/08/多层感知机-手写识别)一样，省略部分
```python
import numpy as np     #导入numpy工具包
from os import listdir #使用listdir模块，用于访问本地文件
from sklearn import neighbors
...
# 构建KNN分类器：设置查找算法以及邻居点数量(k)值。
#- KNN是一种懒惰学习法，没有学习过程，只在预测时去查找最近邻的点，
数据集的输入就是构建KNN分类器的过程。
#- 构建KNN时我们同时调用fit函数。
knn = neighbors.KNeighborsClassifier(algorithm='kd_tree', n_neighbors=3)
knn.fit(train_dataSet, train_hwLabels)
...
```

## 实验效果
### 邻居数量 K 影响分析
设置 K 为1、3、5、7的KNN分类器，对比他们的实验效果
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508224153.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">KNN实验效果</div>
</center>

- K=3时正确率最高，当 K>3 时正确率开始下降，这是由于当样本为稀疏数据集时（本实例只有 946 个样本），其第 k 个邻居点可能与测试点距离较远，因此投出了错误的一票进而影响了最终预测结果。

### 与神经网络对比
将效果最好的 KNN 分类器（K=3）和效果最差的 KNN 分类器（K=7）与各个 MLP 分类器作对比如下：
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);display:inline;margin:0" 
    src="https://cdn.jsdelivr.net/gh/DSzhongweizi/Resources/article/20200508224643.png" width=300 />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;">KNN与神经网络对比</div>
</center>

## 结论：
- KNN 的准确率远高于 MLP 分类器，这是由于 **MLP 在小数据集上容易过拟合**的原因。
- **MLP 对于参数的调整比较敏感**，若参数设置不合理，容易得到较差的分类效果，因此参数的设置对于 MLP 至关重要。